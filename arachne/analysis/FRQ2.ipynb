{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4869f851",
   "metadata": {},
   "source": [
    "The ultimate objective of RQ2 is whether Arachne can generate patches that fix the \"given\" misbehaviour of a DNN. In the traditional APR with generate-and-validate (G&V) approach, we don't consider whether the current fixes can fix the unseen program failure that may take place in future, in general. Similarly, here, we want to investigate the pure capability of Arachne to adjust model not to have the given misbehaviour. However, since the inherent nature of DNNs and that of general software programs are different, we also look into the \"generality\" of the patches, which takes place in RQ3 (not in RQ2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9819c4",
   "metadata": {},
   "source": [
    "Question to answer here: can arachne generate patches directly applied to the DNN's neural weights for the observed misclassification? In other words, here, we investigate the pure capability of Arahcne adjusting the trained model not to have certain misbehaviour. \n",
    "(RQ3 will then examine whether Arachne can generat patches using unseen inputs and whether these synethesised patches can be generalisable to inputs unseen during the training and also during the patch generation? In other words, is Arachne generasible to completely unseen inputs?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230322a",
   "metadata": {},
   "source": [
    "\n",
    "### Overall Repair Rate: a repair rate over the entire faulty inputs\n",
    "### Target Repair Rate: a repair rate over the sampled 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c91374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import utils.data_util as data_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb1c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197391b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to \"results\" to \"/volume1/coinse/Arachne/results/rev\"\n",
    "test_resultdir = \"/Users/jeongju.sohn/workdir/arachne/arachne/results/rq2/on_test\"\n",
    "\n",
    "# train_resultdir -> contains the results when we run the experiments with the past setting \n",
    "#(i.e., repair using the training ddata)\n",
    "train_resultdir = \"/Users/jeongju.sohn/workdir/arachne/arachne/results/rq2/on_train\"\n",
    "indexdir = \"/Users/jeongju.sohn/workdir/arachne/arachne/indices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd4e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_pred_file(which_data, on_test = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if which_data == \"fashion_mnist\":\n",
    "        return os.path.join(indexdir, \n",
    "                            \"fm/test/fashion_mnist.init_pred.indices.csv\" \n",
    "                            if on_test else \"fm/fashion_mnist.init_pred.indices.csv\")\n",
    "    elif which_data == 'cifar10':\n",
    "        return os.path.join(indexdir, \n",
    "                            \"cm/test/cifar10.init_pred.indices.csv\" \n",
    "                            if on_test else \"cm/cifar10.init_pred.indices.csv\" )\n",
    "    elif which_data == 'GTSRB':\n",
    "        return os.path.join(indexdir, \n",
    "                            \"GTSRB/rq1/wo_hist/test/GTSRB.init_pred.indices.csv\" \n",
    "                            if on_test else \"GTSRB/rq1/wo_hist/GTSRB.init_pred.indices.csv\")\n",
    "    else:# imdb\n",
    "        print (\"Wrong data: {}\".format(which_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29da8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(resultdir, which_data, loc, loc_abbr, seeds):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert loc in ['localiser', 'gradient_loss', 'random'], loc\n",
    "\n",
    "    if which_data == 'fashion_mnist':\n",
    "        datadir = os.path.join(resultdir, \"fm/{}/pred\".format(loc))     \n",
    "    elif which_data == 'cifar10':\n",
    "        datadir = os.path.join(resultdir, \"cm/{}/pred\".format(loc))\n",
    "    else: #if which_data == 'GTSRB':\n",
    "        datadir = os.path.join(resultdir, \"gtsrb/{}/pred\".format(loc)) \n",
    "        \n",
    "    predictions = {}\n",
    "    for seed in seeds:\n",
    "        #if not on_eval:\n",
    "        fpath = os.path.join(datadir, \"pred.{}.{}.train.pkl\".format(loc_abbr, seed))\n",
    "        #else:\n",
    "        #    assert 'train' in resultdir, resultdir\n",
    "        #    fpath = os.path.join(datadir, \"pred.{}.{}.eval.pkl\".format(loc_abbr, seed)) \n",
    "        predictions[seed] = pd.read_pickle(fpath)        \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4329460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_br_rr(result_df, seed, index_file):\n",
    "    \"\"\"\n",
    "    broken: initally correct -> incorrect\n",
    "    repair: initially incorrect -> correct\n",
    "    \"\"\"\n",
    "    # get the indices to the initally targeted incorrect input\n",
    "    indices_to_targeted = data_util.get_misclf_for_rq2(index_file, percent = 0.1, seed = seed)\n",
    "    \n",
    "    num_data = len(result_df)\n",
    "    init_incorrect_df = result_df.loc[result_df.init_flag == False]\n",
    "    init_correct_df = result_df.loc[result_df.init_flag == True]\n",
    "    init_num_incorrect = len(init_incorrect_df)\n",
    "    init_num_correct = len(init_correct_df)\n",
    "    \n",
    "    patched_df = init_incorrect_df.loc[init_incorrect_df.true == init_incorrect_df.new_pred]\n",
    "    broken_df = init_correct_df.loc[init_correct_df.true != init_correct_df.new_pred]\n",
    "    \n",
    "    # repair rate -> overall needed b/c we sample 10\\% of the entire misclassified inputs\n",
    "    overall_rr = len(patched_df)/init_num_incorrect\n",
    "    \n",
    "    ## only for the targeted\n",
    "    target_patched_df = patched_df.loc[patched_df.index.isin(indices_to_targeted)]\n",
    "    target_rr = len(target_patched_df)/len(indices_to_targeted)\n",
    "    \n",
    "    # broken rate\n",
    "    br = len(broken_df)/init_num_correct\n",
    "    \n",
    "    # additional\n",
    "    init_acc = np.sum(result_df.true == result_df.pred)/len(result_df)\n",
    "    aft_acc = np.sum(result_df.true == result_df.new_pred)/len(result_df)\n",
    "    acc_r = aft_acc/init_acc\n",
    "    \n",
    "    return {'overall_rr':overall_rr, 'target_rr':target_rr, 'br':br, 'acc_r':acc_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "218b99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_compute_br_rr(resultdir, which_data, loc, seeds = np.arange(30)):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if loc in 'localiser':\n",
    "        loc_abbr = 'loc'\n",
    "    elif loc in 'gradient_loss':\n",
    "        loc_abbr = 'gl'\n",
    "    else:\n",
    "        loc_abbr = 'rd'\n",
    "\n",
    "    predictions_df = get_predictions(resultdir, which_data, loc, loc_abbr, seeds)\n",
    "    index_file = get_init_pred_file(which_data, on_test = 'test' in resultdir)\n",
    "    \n",
    "    rates_of_all = {}\n",
    "    for seed in tqdm(seeds):\n",
    "        rates = compute_br_rr(predictions_df[seed], seed, index_file)\n",
    "        rates_of_all[seed] = rates\n",
    "    \n",
    "    return pd.DataFrame(rates_of_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6bff3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analy_br_rr(overall_rates_df, decimals = 3):\n",
    "    \"\"\"\n",
    "    compute the average br and rr\n",
    "    \"\"\"\n",
    "    avgs = overall_rates_df.mean().to_dict()\n",
    "        \n",
    "    print (\"Overall RR:{}, Target RR:{}, Broken Rate: {}, Acc Rate: {}\".format(\n",
    "        np.round(avgs['overall_rr'], decimals = decimals),\n",
    "        np.round(avgs['target_rr'], decimals = decimals),\n",
    "        np.round(avgs['br'], decimals = decimals), \n",
    "        np.round(avgs['acc_r'], decimals = decimals)))\n",
    "    \n",
    "    #print (np.round(avgs['overall_rr'], decimals = decimals), \n",
    "    #       np.round(avgs['target_rr'], decimals = decimals),\n",
    "    #       np.round(avgs['br'], decimals = decimals))\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788fdff",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7406bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.arange(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b2c63",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9f30e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 86.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 112.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 112.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tst_fm_rates_of_loc = main_compute_br_rr(test_resultdir, 'fashion_mnist', 'localiser', seeds = seeds)\n",
    "tst_fm_rates_of_gl = main_compute_br_rr(test_resultdir, 'fashion_mnist', 'gradient_loss', seeds = seeds)\n",
    "tst_fm_rates_of_rd = main_compute_br_rr(test_resultdir, 'fashion_mnist', 'random', seeds = seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f711c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 45.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 46.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 51.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_fm_rates_of_loc = main_compute_br_rr(train_resultdir, 'fashion_mnist', 'localiser', seeds = seeds)\n",
    "train_fm_rates_of_gl = main_compute_br_rr(train_resultdir, 'fashion_mnist', 'gradient_loss', seeds = seeds)\n",
    "train_fm_rates_of_rd = main_compute_br_rr(train_resultdir, 'fashion_mnist', 'random', seeds = seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9efe0c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.0423, Target RR:0.0866, Broken Rate: 0.007, Acc Rate: 0.9981\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.0473, Target RR:0.095, Broken Rate: 0.0079, Acc Rate: 0.9978\n",
      "\n",
      "Random\n",
      "Overall RR:0.0206, Target RR:0.0542, Broken Rate: 0.0032, Acc Rate: 0.9993\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(tst_fm_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(tst_fm_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(tst_fm_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4746e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.1126, Target RR:0.135, Broken Rate: 0.0068, Acc Rate: 0.9983\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.131, Target RR:0.1503, Broken Rate: 0.0103, Acc Rate: 0.9956\n",
      "\n",
      "Random\n",
      "Overall RR:0.0311, Target RR:0.0552, Broken Rate: 0.0012, Acc Rate: 1.0002\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(train_fm_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(train_fm_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(train_fm_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420dc9f",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd9166d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 144.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 114.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 117.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tst_c10_rates_of_loc = main_compute_br_rr(test_resultdir, 'cifar10', 'localiser', seeds = np.arange(30))\n",
    "tst_c10_rates_of_gl = main_compute_br_rr(test_resultdir, 'cifar10', 'gradient_loss', seeds = np.arange(30))\n",
    "tst_c10_rates_of_rd = main_compute_br_rr(test_resultdir, 'cifar10', 'random', seeds = np.arange(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0dbaaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 60.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 47.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 53.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_c10_rates_of_loc = main_compute_br_rr(train_resultdir, 'cifar10', 'localiser', seeds = np.arange(30))\n",
    "train_c10_rates_of_gl = main_compute_br_rr(train_resultdir, 'cifar10', 'gradient_loss', seeds = np.arange(30))\n",
    "train_c10_rates_of_rd = main_compute_br_rr(train_resultdir, 'cifar10', 'random', seeds = np.arange(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81680333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.0742, Target RR:0.1083, Broken Rate: 0.0305, Acc Rate: 0.998\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.1169, Target RR:0.1357, Broken Rate: 0.0593, Acc Rate: 0.9857\n",
      "\n",
      "Random\n",
      "Overall RR:0.0066, Target RR:0.0174, Broken Rate: 0.0013, Acc Rate: 1.0013\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(tst_c10_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(tst_c10_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(tst_c10_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9dbcf004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.1278, Target RR:0.1463, Broken Rate: 0.0104, Acc Rate: 1.0028\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.1597, Target RR:0.1673, Broken Rate: 0.0251, Acc Rate: 0.9914\n",
      "\n",
      "Random\n",
      "Overall RR:0.0148, Target RR:0.0289, Broken Rate: 0.0008, Acc Rate: 1.0007\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(train_c10_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(train_c10_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(train_c10_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4102a7",
   "metadata": {},
   "source": [
    "## GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39ff9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 122.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 101.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 112.43it/s]\n"
     ]
    }
   ],
   "source": [
    "tst_gtsrb_rates_of_loc = main_compute_br_rr(test_resultdir, 'GTSRB', 'localiser', seeds = np.arange(30))\n",
    "tst_gtsrb_rates_of_gl = main_compute_br_rr(test_resultdir, 'GTSRB', 'gradient_loss', seeds = np.arange(30))\n",
    "tst_gtsrb_rates_of_rd = main_compute_br_rr(test_resultdir, 'GTSRB', 'random', seeds = np.arange(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87161f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 68.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 73.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 50.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_gtsrb_rates_of_loc = main_compute_br_rr(train_resultdir, 'GTSRB', 'localiser', seeds = np.arange(30))\n",
    "train_gtsrb_rates_of_gl = main_compute_br_rr(train_resultdir, 'GTSRB', 'gradient_loss', seeds = np.arange(30))\n",
    "train_gtsrb_rates_of_rd = main_compute_br_rr(train_resultdir, 'GTSRB', 'random', seeds = np.arange(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f102a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.0692, Target RR:0.1304, Broken Rate: 0.007, Acc Rate: 1.0007\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.0683, Target RR:0.1067, Broken Rate: 0.0082, Acc Rate: 0.9993\n",
      "\n",
      "Random\n",
      "Overall RR:0.0039, Target RR:0.0093, Broken Rate: 0.0, Acc Rate: 1.0004\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(tst_gtsrb_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(tst_gtsrb_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(tst_gtsrb_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54a94c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localiser\n",
      "Overall RR:0.75, Target RR:0.7, Broken Rate: 0.0, Acc Rate: 1.0001\n",
      "\n",
      "Gradient loss\n",
      "Overall RR:0.7083, Target RR:0.6667, Broken Rate: 0.0001, Acc Rate: 1.0\n",
      "\n",
      "Random\n",
      "Overall RR:0.0083, Target RR:0.0, Broken Rate: 0.0, Acc Rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "print (\"Localiser\")\n",
    "_ = analy_br_rr(train_gtsrb_rates_of_loc, decimals = 4)\n",
    "print (\"\\nGradient loss\")\n",
    "_ = analy_br_rr(train_gtsrb_rates_of_gl, decimals = 4)\n",
    "print (\"\\nRandom\")\n",
    "_ = analy_br_rr(train_gtsrb_rates_of_rd, decimals = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a072a1a",
   "metadata": {},
   "source": [
    "Overall, I think these results can be used to suppor the claim that Arachne can localise and fix, \n",
    "then, in RQ3, emphasise that the main target or the main goal of Arachne is to generate hot-fixes of a specific type ..\n",
    "+ think taking only 10% may affect here...\n",
    "+ did I take the same number of pos? (anw:  for loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
